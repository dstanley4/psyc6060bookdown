# Making your data ready for analysis

```{r, include=FALSE}
library(tidyverse)
```

## Required Packages

This chapter requires the following packages are installed:

| Required Packages |
|-------------------|
|apaTables          |
|janitor            |
|psych              |
|tidyverse          |

**Important Note:** that you should NOT use library(psych) at any point. There are major conflicts between the psych package and the tidyverse. We will access the psych package command by preceding each command with psych:: instead of using library(psych).

## Objective

Advice is not right or the best - but it is a system. Encourage you to evolve your own system. But perhaps not in the first few months/years of using R until you see the benefits of this system - and correspondingly it's shortcomings.

Warning you need mastery of the Handing data in the tidyvsere chapter before doing this chapter.


## Context

Due to a number of high profile failure to replicate study results [@cos2015] it's become increasingly clear that there is a general crisis of confidence in many areas of science [@baker2016]. Statistical (and other) explanations have been offered [@simmons2011] for why it's hard to replicate results across different sets of data. However, scientists are also finding it challenging to recreate the numbers in their papers using their own data. Indeed, the editor of Molecular Brain asked authors to submit the data used to create the numbers in published papers and found that the wrong data was submitted for 40 out of 41 papers [@miyakawa2020].

Consequently, some researchers have suggested that it is critical to distinguish between replication and reproducibility [@patil2019]. Replication refers to trying to obtain the same result from a different data sets. Reproducibility refers to trying to obtain the same results from the same data set. Unfortunately, some authors use these two terms interchangeably and fail to make any distinction between them. I encourage you to make the distinction and the use the terms consist with use suggested by [@patil2019].

It may seem that reproducibility should be given - but it's not. Correspondingly, there a trend is for journals and authors to adopt Transparency and Openness Promotion (TOP) [guidelines](https://www.cos.io/our-services/top-guidelines
). These guidelines involve such things as making your materials, data, code, and analysis scripts available on public repositories so anyone can check your data. A new open science journal rating system has even emerged called the [TOP Factor](https://topfactor.org
).

The idea is not that open science articles are  more trustworthy that other types of articles -- the idea is that trust doesn't play a role. Anyone can inspect the data using the scripts and data provided by authors. It's really just the same as making your science available for auditing the way financial records can be audited. But just like in the world of business some people don't like the idea of make it possible for other to audit their work. The problems reported at Molecular Brain (doubtless is common to many journals) are likely avoided with open science - because the data and scripts needed to reproduce the numbers in the articles are uploaded prior to publication. 

The TOP open science guidelines have made an impact and some newer journals, such as Meta Psychology, have fully embraced open science. Figure \@ref(fig:metapsychology) shows the header from an [article](https://open.lnu.se/index.php/metapsychology/article/view/1630/2266) in Meta Psychology that clearly delineates the open science attributes of the article that used computer simulations (instead of participant data). Take note that the header even specifies who checked that the analyses in the article were reproducible.

```{r metapsychology, echo=FALSE, out.width="100%", fig.cap="Open science in an article header"}
knitr::include_graphics("ch_score_items/images/screenshot_metapsychology.png")
```

In Canada, the majority of university research is funded by the Federal Government's Tri-Agency (i.e., NSERC, SSHRC, CIHR). The agency has a new draft [Data Management Policy](https://www.ic.gc.ca/eic/site/063.nsf/eng/h_83F7624E.html) in which they state that "*The agencies believe that research data collected with the use of public funds belong, to the fullest extent possible, in the public domain and available for reuse by others.*" This perspective of the funding agency differs from that of some researchers who incorrectly believe "they own their data". In Canada at least, the government makes it clear that tax payers fund the research so the data is public property. Additionally the Tri-Agency Data Management policy clearly indicates the responsibilities of funded researchers:

"Responsibilities of researchers include:

- incorporating data management best practices into their research;
- developing data management plans to guide the responsible collection, formatting, preservation and sharing of their data throughout the entire life cycle of a research project and beyond;
- following the requirements of applicable institutional and/or funding agency policies and professional or disciplinary standards;
- acknowledging and citing data sets that contribute to their research; and
- staying abreast of standards and expectations of their disciplinary community."

As a result of this perspective on data, it's important that you think about structuring your data for reuse by yourself and others before you collect it. Toward this end, you will see documentation of your data file via data code books is critical.


## Begin with the end in mind

In this chapter we will walk you though the steps from data collection, data entry, loading raw data, and the creation of data you will analyze,  analytic data, via preprocessing scripts. These steps are outlined in Figure \@ref(fig:pipeline). This figure makes a clear distinction between raw data and analytic data. Raw data refers to the data as you entered it into a spreadsheet or received it from survey software. Analytic data the data the has been structured and processed so that it is ready for analysis. This pre-processing could include such things as identifying categorical variables to the computer, combining multiple items measuring the same thing into scale scale scores, among other tasks.

It's critical that you don't think of analysis of your results as being completely removed from the data collection and data entry choices you make. Poor choices at the data collection and data entry stage can make your life substantially more complicated when it comes time to write the preprocessing script that will convert your raw data to analytic data. The mantra of this chapter is *begin with the end in mind*.


```{r pipeline, echo = FALSE, out.width="85%", fig.cap = "Data science pipeline by Roger Peng."}
knitr::include_graphics("ch_enter_load/images/pipeline.png")
```


It's difficult to being with the end in mind when you haven't read later chapters. So here we will be provide you with some general thoughts around different approaches to structuring data files and the naming conventions you use when creating those data files.

### Structuring data: Obtaining tidy data

When conducting analyses in R it is typically necessary to have data in a format called tidy data [@tidy-data]. [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) as defined by Hadley involves (among other requirements) that: 

1. Each variable forms a column.
2. Each observation forms a row.

The tidy data format can be initially challenging for some researchers to understand because it is based on thinking about, and structuring data, in terms of observations/measurements instead of participants. In this section we will describe common approaches to entering animal and human participant data and how they can be done keeping the tidy data requirement in mind. It's not essential that data be entered in a tidy data format but it is essential that you enter data in manner that makes it easy to later convert data to a tidy data format.  When dealing with animal or human participant data it's common to enter data into a spreadsheet. Each row of the spreadsheet is typically used to represent a single participant and each column of the spreadsheet is used to represent a variable.

**Between participant data**. Consider Table \@ref(tab:betweenex) which illustrates between participant data for six human participants running 5 kilometers. The first column is id, which indicates there are six unique participants and provides and identification number for each of them. The id is the variable and there is one observation per row - so the id column conforms to tidy data specification. The second column is sex, which is a variable, and there is one observation per for row, so sex also conforms to the tidy data specification. Finally, there is a last column five_km_time which is a variable with one observation per row -- also conforming to tidy data specification. Thus, single occasion between subject data like this conforms to the tidy data specification. There is usually nothing you need to do to convert between participant data (or cross-sectional data) to be in a tidy data format.


```{r betweenex, echo = FALSE}
data_between <- read_csv("data_ex_between.csv", col_type = cols())
knitr::kable(
  head(data_between), caption = 'Between participant data entered one row per participant',
  booktabs = TRUE,
  linesep = c("","","","","","","\\addlinespace")
)
```


**Within participant data**. Consider  Table \@ref(tab:withinex) which illustrates within participant data for six human participants running 5 kilometers - but on three different occasions. The first column is id, which indicates there are six unique participants and provides and identification number for each of them. The id is the variable and there is one observation per row - so the id column conforms to tidy data specification. The second column is sex, which is a variable, and there is one observation per for row, so sex also conforms to the tidy data specification. Next, there are three different columns (march, may, july) representing the levels of a single variable. That is the within subject variable is occasion and the levels of that variable are march, june, and july. The march column contains the times for participants in March. The may column contains the times for participants in May. The july column contains the times for participants in July, These three columns are not in a tidy data format. 


```{r withinex, echo = FALSE}
data_within <- read_csv("data_ex_within.csv", col_types = cols())
knitr::kable(
  head(data_within), caption = 'Within participant data entered one row per participant',
  booktabs = TRUE,
  linesep = c("","","","","","","\\addlinespace")
)
```

```{r, include=FALSE}
data_within_tidy <- data_within %>% 
  pivot_longer(cols = march:july,
               names_to = "occasion",
               values_to =  "elapsed_time")

```

```{r withintidyex, echo = FALSE}
knitr::kable(
  head(data_within_tidy,18), caption = 'A tidy data version of the within participant data',
  booktabs = TRUE,
  linesep = c("", "","\\addlinespace")
)
```


The problem with the format of the data in Table \@ref(tab:withinex) is that march, may, and july are levels of single variable, occasion, that is not represented in the data. Nowhere in the Table \@ref(tab:withinex) can you see the label occasion. This single variable is presented over three columns - a very confusing situation. Moreover, due to the way the columns are labeled it's not clear what is being measured. Nowhere in  Table \@ref(tab:withinex) can you see the variable elapsed_time. Thus with the format used in \@ref(tab:withinex) you don't know what the predictor (occasion) is nor do you know the dependent variable (elapsed_time.). Thus, a major problem with entering data in this format is that there are hidden variables in the data and you need insider knowledge to know the columns represent. That said, this is not necessarily a terrible way to enter your data as long as you have all of this missing information documented in a data code book.


| Disadvantages one row per participant  | Advantages one row per participant | 
| ---------------|------------|
| 1) Predictor variable (*occasion*) is hidden and spread over multiple columns | 1) Easy to enter this way | 
| 2) Unclear that each month is a level of the predictor variable *occasion* | |
| 3) Dependent variable (*elapsed_time*) is not  indicated    | |
| 4) Unclear that *elapsed_time* is the measurement in each month column | |


Fortunately, the problems with Table \@ref(tab:withinex) can be largely resolved by converting the data to the a tidy data format. This can be done with the pivot_long() command that we will learn about later in the chapter. Thus, we can enter the data in the easy to enter format of Table \@ref(tab:withinex) but then later convert it to a tidy data format. After this conversion the data will be appear as in Table \@ref(tab:withintidyex). For elapsed_time variable this data is now in the tidy data format. Each row corresponds to a single elapsed_time observed. Each column corresponds to a single variable. Somewhat problematically, however, sex is repeated three times for each person (i.e., over the three rows) - and this can be confusing. However, if the focus in on analyzing elapsed time this tidy data format makes sense. Importantly, there is an id column for each participant so R knows that this information is repeated for each participant and is not confused by the repeating the sex designation over three rows. In directly, this illustrates the importance of having an id column to indicate each unique participant.

Why did we walk you through this technical treatment of structuring data within the computer at this point in time? So that you pay attention to the advice the follows. You can see at this point that you may well need to restructure your data for certain analyses. The ability to do so quickly and easily depends upon you following the advice in the rest of this chapter around the naming conventions for variables and other aspects of your analyses. You can imagine the challenges for converting the data in Figure \@ref(tab:withinex) to the data in Figure \@ref(tab:withintidyex) by hand. You want to be able to automate that process and others - which is made substantially easier if you following the forthcoming advice about naming conventions in the tidyverse.


## Data collection and entry

Data can be collected in a wide variety of way. Regardless of the methods of location researchers typically come to data in one of two way: 1) a research assistant enters the data into a spreadsheet type interface, or 2) the data is obtained as the output from computer software (e.g., Qualtrics, SurveyMonkey, Noldus, etc.). Regardless of the approach it is critical to name your variables appropriately. For spreadsheet users, this mean setting of the spreadsheet the data will be recorded in with column names that are amenable to the future analyses you want to conduct. Likewise, for software uses, this means setting up the software prior to running the experiment/survey to that when the data is exported for analysis the column names you obtain are amenable to subsequent analyses approaches. Although failure to take this thoughtful approach in advance can be overcome - it is only overcome with substantial manual effort. Therefore, as noted previously, we strongly encourage you to following the naming conventions we espouse here where you set up your data recording regime. Additionally, we encourage you to give careful thought in advance to the codes you will use to record missing data.

### Naming conventions

To make your life easier down the road, it is critical you set up your spreadsheet or online survey such that is uses a naming convention prior to data collection. The naming conventions suggested here are adapted from the tidyverse [style guide](https://style.tidyverse.org).


* Lowercase letters only

* If two word column names are necessary, only use the underscore ("_") character to separate words in the name.

* Avoid short uncontextualized variable names like q1, q2, q3, etc.

* Do use moderate length column names. Aim to achieve a unique prefix for related columns so that those columns can be selected using the starts_with() command. Be sure to avoid short two or three letter prefixes for item names. Use moderate length unique item prefixes so that it will easy to select with those columns using start_with() in that you don't accidentally get additionally columns you don't want - that have a similar prefix. See Likert-type item section below for details.

* If you have a column name that represents levels of two repeated measures variables only use the underscore character to separate the levels of the different variables. See within-participant ANOVA section below for details.

### Likert-type items

A Likert-type item is typically composed of a statement that participants are asked to agree or disagree with. For example, participants could be asked to indicate the extent to which they agree a number of statements such as "I like my job". They would then be presented with response scale such as: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. A common question is how should I enter the data?

* **Enter numeric responses not the labels**. You should enter the numeric value for each item response (e.g., 5) into your data - not the label (e.g., Strongly Agree). The labels associated with each value can be applied later in a script, if needed.

* **High numbers should be associated with more of the construct being measured.** When designing your survey or data collection tools, it is important that you set of the response options appropriately. If you scale measures job satisfaction, it is important that you collect data in a manner that ensures high numbers on the job satisfaction scale indicate high levels of job satisfaction. Therefore, assigning numbers make sense using the 5-point scale: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. With this approach high response numbers indicate more job satisfaction. However, using the opposite scale would not make sense: 1 - Strongly Agree, 2 - Moderately Agree, 3 - Neutral, 4, Moderately Disagree, 5 - Strongly Disagree. With this opposite scale high numbers on a job satisfaction scale would indicate lower levels of job satisfaction - a very confusing situation. Avoid this situation, assign numbers so that higher numbers are associated with more of the construct being measured. 

* **Use appropriate item names.** As described in the naming convention section, use moderate length names with different labels for each subscale.

* **Use moderate length column names unique to each subscale**. Imagine you have a survey with an 18-item commitment scale [@meyer1993commitment] composed of three 6-item subscales: affective, normative, and continuance commitment. It would be a poor choice to prefix the labels of all 18 columns in your data with "commit" such that the names would be commit1, commit2, commit3, etc. The problem with this approach is that it fails to distinguishing between the three subscales in naming convention; making it impossible to select the items for a single subscale using starts_with(). A better, but still poor choice for a naming convention would be use use a two letter prefix for the three scale such ac, nc, and cc. This would result in names for the columns like ac1, ac2, ac3, etc. This is an improvement because you could apparently (but likely not) select the columns using starts_with("ac"). The problem with these short names is that there could be many columns in data set that start with "ac" beside the affective commitment items. You might want to select the affective commitment items using starts_with("ac"); but you would get all the affective commitment item columns but also all the columns measuring other variables that also start with "ac". Therefore, it's a good idea to use a moderate length unique prefix for column names. For example, you might use prefixes like affectcom, normcom, and contincom for the three subscales. This would create column names like affectcom1, affectcom2, affectcom3, etc. These column prefixes are unlikely to be duplicated in other places in your column name conventions making it easy to select those columns using a command like starts_with("affectcom").

* **Indicate in the item name if the item is reversed keyed ** Sometimes with Likert-type items, an item is reverse keyed. For example, on a job satisfaction scale participants will typically respond to items that reflect job satisfaction using the scale: 1 - Strongly Disagree, 2 - Moderately Disagree, 3 - Neutral, 4, Moderately Agree, 5 - Strongly Agree. Higher numbers indicate more job satisfaction. Sometimes however, some items will use the same 1 to 5 response scale but be worded "I hate my job". Responding with a 5 to this item would indicate high job dissatisfaction not high job satisfaction - to the response will need to be flipped in your analysis script after data collection (i.e., 1 need to become a 5 and vice versa). To make it easier to do so, you should indicate if an item is reverse keyed in the item name. The procedure for doing so is outlined in the next point.  

* **Indicate in the item name the range for reverse key items. ** If an item is reverse-keyed, the process for the flipping the scores depends upon the range of a scale. Although 5-point scale are common, any number of points are possible. The process for correcting a reverse key item depends upon: 1) the number of points on the scale, and 2) the range of the points on the scale. The reverse-key item correction process is different for an item that uses a 5-point scale ranging from 1 to 5 and from 0 to 4. Both are 5-point scale but your correction process will be different. Therefore, for reverse key items add as suffix at the end of each item name that indicates an item is reverse keyed and the range of the item. For example, if the third job satisfaction item was reversed keyed on scale using a 1 to 5 response format you might name the item: jobsat3_rev15. The suffix "_rev15" indicates the item is reverse keyed and the range of responses used on the item is 1 to 5. Be sure to set up your survey with this naming convention when you collect your data.

* If you collect items over multiple time points use a prefix with a short code to indicate the time followed by and underscore. For example, if you had a multi-item self-esteem scale you might call the column for the firs time "t1_esteem1_rev15". This indicate that you have for time 1 (t1), the first self-esteem item (esteem1) and that item is reverse keyed on a 1 to 5 scale.


### ANOVA between-participant levels

Avoid numerical representation of categorical variables. Don't use 1 or 2 to represent sex. Use male and female in your spreadsheet - likewise in your survey program. Likewise, for between participant variables drug_condition don't use 1 or 2 use "drug" and "placebo" but the actually drug name would be better.s


### ANOVA within-participant levels

If you have a study that involves a within-participant ANOVA. 

One-way repeated measure predictor.  == See example above with run times.

Multi-way repeated measures predictors. If you have a column name that represents levels of two repeated measures variables only use the underscore character to separate the levels of the different variables. For example, if imagine you are a food researcher interested in taste ratings as a dependent various foods and contexts. You have food type (i.e., food_type) as a predictor with three levels (pizza, steak, burger). You have a second predictor temperature with two levels (hot, cold). All participants taste all foods at all temperatures. Thus, six columns are required for each participant: pizza_hot, pizza_cold, steak_hot, steak_cold, burger_hot, and burger_cold. Notice how each name contains one level of each predictor variable. The levels by the two predictor variables are separated by a single underscore. This should the only underscore in the variable name because that underscore will be used by the computer when changing the data to the tidy format. If you had two underscores an name like "italian_pizza_hot" you would confuse the pivot_longer() command when it attempts to create a tidy version of the data. The computer would think there were three repeated levels variables instead of two. Thus, when dealing with repeated measures predictors, only use underscores to separate levels of predictor variables.

### Other types of information.
education, income, etc


### Working the examples

Below we present example scripts transforming raw data to analytic data for various study designs (experimental and survey). These scripts illustrate the value of using the naming conventations outlined previously. Follow along with the projects by placing the data in your R Studio project folder and typing your own script. Resist the urge to cut and paste from this document. After learning and teaching scripting for years, there we've found there is substantial bennefit to the process of simply typing the script yourself.

Setup an R Studio project using one of these approaches:

R Studio in the Cloud Assignment
1. The data should be in the assignment project automatically. Just start the assignment.

R Studio in the Cloud, custom project
1. Create a new Project using the web interface
2. Upload your data files in using the upload button in the Files pane

R Studio Local Computer, custom project
1. Create a folder on your computer for the analysis
2. Place your data files in that folder
3. Use the menu item File > New Project... to start the project
4. On the window that appears select "Existing Directory"
5. On the next screen, press the "Browse" button and find/select the folder with your data
6. Press the Create Project Button

Regardless of whether your are working from the cloud or locally you should now have an R Studio project with your data files in it. Using Projects.

As you read each of these example scripts pay attention to the redundancy. Regardless of the of the type of data you analyze there is a great deal of consistency in terms of how all of the scripts start.

Pay particular attention to XXXX where the reearcher assistant didn't fllow and sex as 1 and 

## Experiment: Between-participant

```{r, results='hide'}
# Date: YYYY-MM-DD
# Name: your name here
# Example: Between-participant experiment

# Load data
library(tidyverse)

my_missing_value_codes <- c("-999", "", "NA")

raw_data_beween <- read_csv(file = "data_ex_between.csv",
                     na = my_missing_value_codes)

analytic_data_between <- raw_data_beween

```

```{r, results='hide'}
library(janitor)

# Initial cleaning
analytic_data_between <- analytic_data_between %>%
  remove_empty("rows") %>%
  remove_empty("cols") %>%
  clean_names()
```

You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column.

```{r}
glimpse(analytic_data_between)
```

### Converting categorical variables to factors

Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it's critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character. We have one variable sex that is categorical that is of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column.


```{r}
glimpse(analytic_data_between)
```

You can quickly convert all character columns to factors using the text below:

```{r}
# Turn all columns that are of type character into factors
analytic_data_between <- analytic_data_between %>%
  mutate(across(.cols = where(is.character),
                .fns = as_factor))

```

The participant identification number in the id column is a numeric column, so we have handle that column on it's own. 

```{r}
analytic_data_between <-analytic_data_between %>%
  mutate(id = as.factor(id))
```

You can ensure all of these columns are now factors using the glimpse() command. 

```{r}
glimpse(analytic_data_between)
```

Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors.


### Levels of factors

The order of the levels of factors influences how text output and graphs are generated. You can see the order of the factor levels using:

```{r}
analytic_data_between %>%
  select(where(is.factor)) %>%
  summary()
```

The sex column has two levels: male and female in that order. Below we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. 
```{r}
# Custom reordering of factor levels
analytic_data_between <- analytic_data_between %>%
  mutate(sex = fct_relevel(sex,
                           "female",
                           "male"))
```


You can see the new order of the factor levels with summary():
```{r}
analytic_data_between %>%
  select(where(is.factor)) %>%
  summary()
```

You now have a between-participant analytic data set.

## Experiment: Within one-way

```{r, results='hide'}
# Date: YYYY-MM-DD
# Name: your name here
# Example: Within-participant experiment

# Load data
library(tidyverse)

my_missing_value_codes <- c("-999", "", "NA")

raw_data_within <- read_csv(file = "data_ex_within.csv",
                     na = my_missing_value_codes)

analytic_data_within <- raw_data_within

```


```{r, results='hide'}
library(janitor)

# Initial cleaning
analytic_data_within <- analytic_data_within %>%
  remove_empty("rows") %>%
  remove_empty("cols") %>%
  clean_names()
```

You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column.

```{r}
glimpse(analytic_data_within)
```

### Converting categorical variables to factors

Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it's critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character. We have one variable sex that is categorical that is of type character (i.e., chr). The participant id column is categorical as well, but of type double (i.e., dbl) which is a numeric column.

Importantly, note that we have a categorical variable that is missing because our data is not in the tidy data format. The predictor "occassion" (levels: march, may, and july) is missing. The levels for occasion are distributed over three columns. We will have to fix this later. For now we focus on the sex and id columns.


```{r}
glimpse(analytic_data_within)
```

You can quickly convert all character columns to factors using the text below. 

```{r}
# Turn all columns that are of type character into factors
analytic_data_within <- analytic_data_within %>%
  mutate(across(.cols = where(is.character),
                .fns = as_factor))

```

The participant identification number in the id column is a numeric column, so we have handle that column on it's own. 

```{r}
analytic_data_within <-analytic_data_within %>%
  mutate(id = as.factor(id))
```

You can ensure all of these columns are now factors using the glimpse() command. 

```{r}
glimpse(analytic_data_within)
```

Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors.


### Levels of factors

The order of the levels of factors influences how text output and graphs are generated. You can see the order of the factor levels using:

```{r}
analytic_data_within %>%
  select(where(is.factor)) %>%
  summary()
```


Here we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: female, male. 

```{r}
# Custom reordering of factor levels
analytic_data_within <- analytic_data_within %>%
  mutate(sex = fct_relevel(sex,
                           "female",
                           "male"))
```

You can see the new order of the factor levels with summary():
```{r}
analytic_data_within %>%
  select(where(is.factor)) %>%
  summary()
```

### Pivot to tidy data

```{r}
print(analytic_data_within)
```


```{r}
analytic_data_within_tidy <- analytic_data_within %>%
  pivot_longer(march:july,
               names_to = "occasion",
               values_to = "elapsed_time"
  )
```

```{r}
print(analytic_data_within_tidy)
```


```{r}
analytic_data_within_tidy <-analytic_data_within_tidy %>%
  mutate(occasion = as.factor(occasion))
```

```{r}
glimpse(analytic_data_within)
```

## Experiment: Within N-way

```{r, results='hide'}
# Date: YYYY-MM-DD
# Name: your name here
# Example: Within-participant experiment

# Load data
library(tidyverse)

my_missing_value_codes <- c("-999", "", "NA")

raw_data_food <- read_csv(file = "data_food.csv",
                     na = my_missing_value_codes)

analytic_data_food <- raw_data_food

```

```{r, results='hide'}
library(janitor)

# Initial cleaning
analytic_data_food <- analytic_data_food %>%
  remove_empty("rows") %>%
  remove_empty("cols") %>%
  clean_names()
```

You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column.

```{r}
glimpse(analytic_data_food)
```

### Converting categorical variables to factors

Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it's critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character


```{r}
glimpse(analytic_data_food)
```

You can quickly convert all character columns to factors using the text below:

```{r}
# Turn all columns that are of type character into factors
analytic_data_food <- analytic_data_food %>%
  mutate(across(.cols = where(is.character),
                .fns = as_factor))

```

The participant identification number in the id column is a numeric column, so we have handle that column on it's own. 

```{r}
analytic_data_food <-analytic_data_food %>%
  mutate(id = as.factor(id))
```

You can ensure all of these columns are now factors using the glimpse() command. 

```{r}
glimpse(analytic_data_food)
```

Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors.


### Levels of factors

The order of the levels of factors influences how text output and graphs are generated. You can see the order of the factor levels using:

```{r}
analytic_data_food %>%
  select(where(is.factor)) %>%
  summary()
```


Here we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: intersex, female, male. 

```{r, eval = FALSE}
# Custom reordering of factor levels
analytic_data_food <- analytic_data_food %>%
  mutate(sex = fct_relevel(sex,
                           "female",
                           "male"))
```


You can see the new order of the factor levels with summary():
```{r}
analytic_data_food %>%
  select(where(is.factor)) %>%
  summary()
```


```{r}
analytic_food_tidy <- analytic_data_food %>%
  pivot_longer(pizza_hot:burger_cold,
               names_to = c("food", "temperature"),
               names_sep = "_",
               values_to = "taste"
  )
```

```{r}
print(analytic_food_tidy)
```

```{r}

# Turn all columns that are of type character into factors
analytic_food_tidy <- analytic_food_tidy %>%
  mutate(across(.cols = where(is.character),
                .fns = as_factor))

# sex is numeric so must be done the hard way
analytic_food_tidy <- analytic_food_tidy %>%
  mutate(sex = as_factor(sex))

# establish that 1 indicates males and 2 indicates females
analytic_food_tidy <- analytic_food_tidy %>%
  mutate(sex = fct_recode(sex, 
                          male = "1",
                          female = "2"))


# see order of factors
analytic_food_tidy %>%
  select(where(is.factor)) %>%
  summary()

# set order of factors for x-axis or legend
analytic_food_tidy <- analytic_food_tidy %>%
  mutate(sex = fct_relevel(sex,
                           "female",
                           "male"))

# see order of factors
analytic_food_tidy %>%
  select(where(is.factor)) %>%
  summary()

```



## Surveys: Single Occassion

Even assuming you did everything right there is still a fair amount of work to get things started paragraph here...


We begin by examining the data as originally entered into a spreadsheet. In Figure \@ref(fig:rawdataitems) you see a screen shot of the initial raw data as a researcher might receive it. Take careful note of the numerous -999 values used to indicate missing values. As part of creating the analytic data that we will analyze we need to indicate to the computer that the -999 are not data but codes to represent missing values.


```{r rawdataitems, echo = FALSE, out.width="85%", fig.cap = "Raw data for item scoring"}
knitr::include_graphics("ch_score_items/images/screenshot_raw_data.png")
```



 * Create a new script in your project and save it with the name "script_raw_to_analytic.R"
 * Type the code below into that script
 

```{r, results='hide'}
# Date: YYYY-MM-DD
# Name: your name here
# Example: single occasion survey

# Load data
library(tidyverse)

my_missing_value_codes <- c("-999", "", "NA")

raw_data_survey <- read_csv(file = "data_item_scoring.csv",
                     na = my_missing_value_codes)

analytic_data_survey <- raw_data_survey

```

Remove empty row and columns from your data using the remove_empty_cols() and remove_empty_rows(), respectively. As well, clean the names of your columns to ensure they conform to tidyverse naming conventions. 

```{r, results = "hide"}
library(janitor)

# Initial cleaning
analytic_data_survey <- analytic_data_survey %>%
  remove_empty("rows") %>%
  remove_empty("cols") %>%
  clean_names()
```

You can confirm the column names following our naming convention with the glimpse command - and see the data type for each column.

```{r}
glimpse(analytic_data_survey)
```

### Converting categorical variables to factors

Following initial cleaning, identify categorical variables as factors. If you plan to conduct an ANOVA - it's critical that all predictor variables are converted to factors. Inspect glimpse() output - if you followed our data entry naming conventions categorical variables should be of the type character


```{r}
glimpse(analytic_data_survey)
```

You can quickly convert all character columns to factors using the text below:

```{r}
# Turn all columns that are of type character into factors
analytic_data_survey <- analytic_data_survey %>%
  mutate(across(.cols = where(is.character),
                .fns = as_factor))

```

The participant identification number in the id column is a numeric column, so we have handle that column on it's own. 

```{r}
analytic_data_survey <-analytic_data_survey %>%
  mutate(id = as.factor(id))
```

You can ensure all of these columns are now factors using the glimpse() command. 

```{r}
glimpse(analytic_data_survey)
```

Inspect the output of the glimpse() command and make sure you have coverted all categorical variables to factors - especially those you will use as predictors.


### Levels of factors

The order of the levels of factors influences how text output and graphs are generated. You can see the order of the factor levels using:

```{r}
analytic_data_survey %>%
  select(where(is.factor)) %>%
  summary()
```


Here we adjust the order of the sex varible because we want the x-axis of a future graph to display columns in the left to right order: intersex, female, male. 

```{r}
# Custom reordering of factor levels
analytic_data_survey <- analytic_data_survey %>%
  mutate(sex = fct_relevel(sex,
                           "intersex",
                           "female",
                           "male"))
```

For eye color, we want to future graph to be have the most commmon eye colors on the left so we reorder the factor levels:
```{r}
#  Reordering factor levels by frequency
analytic_data_survey <- analytic_data_survey %>%
  mutate(eye_color = fct_infreq(eye_color))
```

You can see the new order of the factor levels with summary():
```{r}
analytic_data_survey %>%
  select(where(is.factor)) %>%
  summary()
```



### Checking numerical variables


```{r, eval = FALSE}
analytic_data_survey %>%
  select(where(is.numeric)) %>%
  summary()
```

```{r, eval = TRUE, echo = FALSE}
analytic_data_survey %>%
  select(esteem1, esteem2, esteem3) %>%
  summary()
```



### Create Scale Scores

Likert caveats. Depending on your research area, there are very different perspectives on how to analyze Likert-type items. The perspectives differ based on how they view the [level of measurement](https://en.wikipedia.org/wiki/Level_of_measurement) associated with the Likert-type items. One perspective, common in Psychology, is that the numbers corresponding to each response option are of the interval type. This mean that the psychological difference between each response option perfectly corresponds to the numerical difference between each response option. That is, the psychological difference between Strongly Agree and Moderately Agree is equal to the psychological difference between Moderately Agree and Neutral, and so on.



Now it looks like our data is ready for the creation of scale scores:

```{r}
glimpse(analytic_data_survey)
```



#### Dealing with reverse key items


Our first step is dealing with reverse key items. The way you deal with these items depends on how you scored them. Imagine you had a 5-point scale. You could have scored the scale with the values 1, 2, 3, 4, and 5. Alternatively, you could have scored the scale with the values 0, 1, 2, 3, and 4. In this example, we scored the data using the 1 to 5 system. So we'll use that. Later I'll show you how to deal with the other scoring system (0 to 4).

### Scoring items where the ratings scale starts with 1
We need to take items that were reversed-key when the participant wrote them and recode those responses. We do that with using the *mutate* command from the *dplyr* package.

In this data file the only reverse-key item was SE7 (we known this from when we created the survey). We use the command below to reverse key an item with response options ranging from 1 to 5. So we use 6 in the command (i.e., one higher than 5).



```{r}
analytic_data_survey %>%
  select(ends_with("_rev15")) %>%
  glimpse()
```

```{r,echo=TRUE,eval=TRUE}
analytic_data_survey <- analytic_data_survey %>% 
  mutate(6 - across(.cols = ends_with("_rev15")) ) %>% 
  rename_with(.fn = str_remove,
              .cols = ends_with("_rev15"),
              pattern = "_rev15")
```
The command above creates a new column in analytic_data called SE7c that has the reverse-keyed values for SE7 in it.  You can see the new SE7c column using command below that displays the first six rows of the data. The SE7c column is at the far right of the data displayed.

```{r,echo=TRUE,eval=TRUE}
glimpse(analytic_data_survey)
```

**Note FOR A BOX **. If you had used response options numbered 0 to 4 for each item you would use the command below instead. Note that we use 4 in the command this time instead of a value one higher.
```{r,echo=TRUE,eval=FALSE}
analytic_data_survey <- mutate(analytic_data_survey, esteem7c = 4 - SE7) 
```

### Creating the scale scores

```{r}
analytic_data_items <- analytic_data_survey
```


```{r}
# confirm select will select the columns you want (and not others)
analytic_data_survey %>%
  select(starts_with("esteem")) %>%
  glimpse()
```

```{r}
# confirm select will select the columns you want (and not others)
analytic_data_survey %>%
  select(starts_with("jobsat")) %>%
  glimpse()
```


```{r}
analytic_data_survey <- analytic_data_survey %>% 
  rowwise() %>% 
  mutate(self_esteem = mean(c_across(starts_with("esteem")),
                               na.rm = TRUE)) %>%
  mutate(job_sat = mean(c_across(starts_with("jobsat")),
                               na.rm = TRUE)) %>%
  ungroup() %>%
  select(-starts_with("esteem")) %>%
  select(-starts_with("jobsat")) 
  
```

When you see ungroup() in this context you can think of it as "turn off rowwise".

We can see our data now has the self esteem column and no esteem items.

```{r}
glimpse(analytic_data_survey)
```



## Surveys: Multiple Occasions

```{r, include=FALSE}
library(tidyverse)

raw_data_survey <- read_csv("data_item_time.csv")

analytic_data_survey <- raw_data_survey

```

```{r, eval=FALSE}
library(tidyverse)

raw_data_survey <- read_csv("data_item_time.csv")

analytic_data_survey <- raw_data_survey

```

```{r}
glimpse(analytic_data_survey)
```

Handle categorical variables:

```{r}
analytic_data_survey <-analytic_data_survey %>%
  mutate(id = as.factor(id))

# Turn all columns that are of type character into factors
analytic_data_survey <- analytic_data_survey %>%
  mutate(across(.cols = where(is.character),
                .fns = as_factor))


# Custom reordering of factor levels for sex
analytic_data_survey <- analytic_data_survey %>%
  mutate(sex = fct_relevel(sex,
                           "intersex",
                           "female",
                           "male"))

#  Reordering factor levels by frequency for eye color
analytic_data_survey <- analytic_data_survey %>%
  mutate(eye_color = fct_infreq(eye_color))
```


Create scale scores:

```{r}

# Reverse code items
analytic_data_survey <- analytic_data_survey %>% 
  mutate(6 - across(.cols = ends_with("_rev15")) ) %>% 
  rename_with(.fn = str_remove,
              .cols = ends_with("_rev15"),
              pattern = "_rev15")


analytic_data_survey <- analytic_data_survey %>% 
  rowwise() %>% 
  mutate(esteem_t1 = mean(c_across(starts_with("t1_esteem")),
                               na.rm = TRUE)) %>%
  mutate(esteem_t2 = mean(c_across(starts_with("t2_esteem")),
                               na.rm = TRUE)) %>%
  mutate(jobsat_t1 = mean(c_across(starts_with("t1_jobsat")),
                               na.rm = TRUE)) %>%
  mutate(jobsat_t2 = mean(c_across(starts_with("t2_jobsat")),
                               na.rm = TRUE)) %>%
  ungroup() %>%
  select(-starts_with("t1_esteem")) %>%
  select(-starts_with("t2_esteem")) %>%
  select(-starts_with("t1_jobsat")) %>%
  select(-starts_with("t2_jobsat"))

```

```{r}
glimpse(analytic_data_survey)
```

But now comes the complicated point wher eyou need to pivot the data to longer.

```{r}
print(analytic_data_survey)
```

```{r}
analytic_survey_tidy <- analytic_data_survey %>%
  pivot_longer(esteem_t1:jobsat_t2,
               names_to = c(".value", "time"),
               names_pattern = "(.*)_(t.)"
  )
```

```{r}
print(analytic_survey_tidy)
```


## Basic descriptive statistics


### The usual suspects

One approach is the describe() command from the psych package.
```{r}
psych::describe(analytic_data_survey)
```


One approach is the apa.cor.table() command from the apaTables package.
```{r}
library(apaTables)
analytic_data_survey %>%
  select(where(is.numeric)) %>%
  apa.cor.table()
```

The tidyverse approach is oddly long but incredibly flexible. More details on why.

```{r}
library(tidyverse)
# HMisc package must be installed. 
# Library command not needed for HMisc package.

desired_descriptives <- list(
  mean = ~mean(.x, na.rm = TRUE),
  CI95_LL = ~Hmisc::smean.cl.normal(.x)[2],
  CI95_UL = ~Hmisc::smean.cl.normal(.x)[3],
  sd = ~sd(.x, na.rm = TRUE),
  min = ~min(.x, na.rm = TRUE),
  max = ~max(.x, na.rm = TRUE),
  n = ~sum(!is.na(.x))
)

row_sum <- analytic_data_survey %>% 
  summarise(across(.cols = where(is.numeric),
                   .fns =  desired_descriptives,
                   .names = "{col}___{fn}"))

long_summary <- row_sum %>%
  pivot_longer(cols = everything(),
               names_to = c("var", "stat"),
               names_sep = c("___"),
               values_to = "value")

summary_table <- long_summary %>% 
  pivot_wider(names_from = stat,
              values_from = value)

summary_table_rounded <- summary_table %>%
  mutate(across(.cols = where(is.numeric),
                .fns= round,
                digits = 3)) %>%
  as.data.frame()

print(summary_table_rounded)
```

### Cronbach’s alpha

If you want Cronbach’s alpha to estimate the reliability of the scale, you can use the alpha command from the psych package with the code below. Note we have to use the item level data we created a copy of, called analytic_data_items. Cronbach’s alpha is labeled “raw alpha” in the output.

```{r, eval = TRUE}
rxx_alpha <- analytic_data_items %>%
  select(starts_with("esteem")) %>%
  psych::alpha()

print(rxx_alpha$total)
```




If you entered data following the style guide recommended here;  entering categorical variables by the category name rather than numerically we can quickly convert categorical variable to factors. For example, if you used a data entry convention where you entered male/female in the column for sex rather than 1 and 2 we can quickly convert all of those columns to factors. This because if you followed the naming convention, columns that contain categorical information will be of the character <chr> type. This is quickly revealed by a glimpse command:

